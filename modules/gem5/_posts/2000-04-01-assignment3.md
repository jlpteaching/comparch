---
Author: Jason Lowe-Power
Editor:  Maryam Babaie
Title: ECS 201A Assignment 3
---

WORK IN PROGRESS!!! PLEASE DO NOT START YET!!

## Table of Contents

## Introduction

In this assignment, you'll be investigating the performance impacts of different out-of-order core designs on a set of RISC-V benchmarks.
The goals of this assignment are:

- Show how applications have different behaviors as the microarchitecture changes.
- Give you experience investigating the *bottleneck* in a particular architecture.
- Improve your understanding of out-of-order processor architecture.

In this assignment, you will be running a set of simple benchmarks (the "Stanford" benchmarks) taken from the [LLVM test-suite](https://github.com/llvm/llvm-test-suite/tree/main/SingleSource/Benchmarks/Stanford).
We have precomiled these for you for the RISC-V ISA, which you can download as part of the [assignment 3 template files]({{ "img/assignment3-template.tgz" | relative_url }}).
You'll be running them on the out-of-order CPU model in gem5 (the "O3CPU", O3->"OOO").

For this assignment, since we're interested only in the kernel of the benchmarks, we'll be using gem5's *syscall emulation* mode.
For this assignment, we don't care about I/O or operating system effects, so we will not use full system.
This means that we won't have to wait for Linux to boot or require a precompiled Linux kernel, etc.

## Compile gem5 to execute RISC-V binaries

First, we need to create a gem5 simulator binary that can execute RISC-V code.
As of the writing of this assignment (Winter 2022), each *target* ISA that you want to run requires a different gem5 build.
To compile gem5 to execute RISC-V binaries, we will use the default build options file for RISC-V: `build_opts/RISCV`.

```sh
scons build/RISCV/gem5.opt -j<your number of cores>
```

Note that this requires 3-4 GiB of disk space, so if you're running on a computer or virtual machine with limited disk space you may need to delete other prior builds (e.g., `rm -r build/X86`).

## Template files and gem5's out-of-order CPU model

In this assignment, we will be using the [gem5 standard library](https://www.gem5.org/documentation/gem5-stdlib/overview) for the simulated system (called a board), to run the simulation via the `Simulator` class, and for the benchmark `Resources`.

Description of the files in the [template zip file]({{ "img/assignment3-template.tgz" | relative_url }}).

- `riscv_se.py`: This file contains a simple RISC-V system with a configurable out-of-order core.
- `stanford_benchmarks.py`: Contains the "resources" for the stanford benchmarks precompiled for RISC-V.
- `run.py`: This file is a simple gem5 run script which takes a parameter of the core (options found in the `cores` dictionary) and the benchmark (as specified in the `stanford_benchmarks.py` file.)
- `Stanford`: the stanford benchmarks. Each benchmark has an associated `.c` file which is the original source, a `.s` file which is the assembly generated by GCC, and the binary file that is executed in gem5. Each binary takes 1-3 minutes to execute in gem5.
- `run.sh`: A simple script to run all configurations for all benchmarks in parallel. By default, this will launch 22 processes, which is probably too many for most systems. Feel free to modify this to execute fewer at a time. This file also makes sure that the output directories for the stats files are all different based on the core name and the benchmark.

### The O3CPU model

You can find some information on the O3CPU in gem5's documentation: <https://www.gem5.org/documentation/general_docs/cpu_models/O3CPU>.

### Options for the core design

#### Pipeline width

This is the width of the pipeline.
By setting this value, you're setting the width of each stage to be the same.

#### ROB size

This is the number of entries in the reorder buffer.
This will constrain the number of instructions which can be "in flight" in the processor.

#### Number of physical registers

This is the number of registers in the *physical* register file.
In order to overcome write-after-read and write-after-write hazards you have to be able to allocate temporary registers.
Register renaming and using a pool a physical register larger than the number of logical registers allows the out-of-order core to track the true read-after-write dependences and break some of the false dependences.

Note that this must be larger than the 32 logical registers in RISC-V or gem5 will hang.

#### Branch predictor

To find more information on these branch predictors, see `gem5/src/cpu/pred/BranchPredictor.py`.

- `simple`: Tournament branch predictor as discussed in class where there's a predictor to choose either a local or global predictor.
- `ltage`: A TAgged GEometric history length predictor. At a high-level, this branch predictor enables much longer (e.g., 100s or 1000s of instructions) global history. See <https://jilp.org/vol8/v8paper1.pdf> for details.
- `perceptron`: This branch predictor uses a table of simple neural networks (well, not a network because it's a single "neuron"). See <https://jilp.org/cbp2016/paper/DanielJimenez1.pdf> for more details.
- `complex`: Combines TAGE and Perceptron predictors.

### Given core configurations

You're given two core configurations.
You will have to extend this with other configurations as you are answering the questions below.

`LargeCore`: This is a core with many resources (some the maximum gem5 supports like being 8-wide). You will be using this as the "maximum" performance core.
`SmallCore`: This is a core with few resources (some the minimum that gem5 supports like only 48 physical registers). You will be using this as the "minimum" performance core.

## Assignment

Answer the following questions.
To answer the questions, you'll have to add other core configurations to the runscript, run experiments, and compare data between the experiments.
Be sure to include any relevant data in your writeup.
However, remember this should be *well-written* technical document, so *only* include relevant data and not extra data that's not needed.

### Part 1: Analyzing the workloads

1. What is the average improvement in IPC of the `LargeCore` compared to the `SmallCore`? (Remember to use the correct average statistic).
2. Some workloads show more speedup that others. Which workloads show high speedup, which show low speedup? Look at the benchmark code (both the `.c` and `.s` files may be useful) and speculate the *algorithm characteristics* which influence the IPC difference between the `LargeCore` and the `SmallCore`. What characteristics do applications have that lead to low performance improvement and what characteristics lead to high performance improvement?
3. Which workload(s) have the highest IPC for the large core? What is unique about this workload?

### Part 2: Analyzing the hardware

Between the `SmallCore` and the `LargeCore`, we are varying many parameters at the same time.
In this part, we will investigate which parameters are most important.
Vary the four parameters in the core configuration: pipeline width, ROB entries, physical register entries, and branch predictor to answer the following questions.

1. Which parameter(s) have the most impact on performance? Is this the same for all workloads or does it vary between different workloads?
2. Are there any dependences between the parameters (e.g., do you need to change two or more parameters at once to have an impact on the performance of different applications)?
3. Pick a core design between the cost of the `SmallCore` and the `LargeCore` which gives most of the performance benefits of the large core but with fewer resources.
    a. What are the parameters you chose? Let's call this the `GoodCore`.
    b. What is the average IPC improvement of your `GoodCore` design compared to the `SmallCore`? c. How much more performance could you get from the `LargeCore`? (What is the average speedup of the `LargeCore` compared to your `GoodCore`)?
    d. Are the workloads that benefited the most from the `LargeCore` the same as the ones that benefit from the `GoodCore`? Why or why not (talk about workload characteristics).
